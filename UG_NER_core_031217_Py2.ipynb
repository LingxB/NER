{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pycrfsuite\n",
    "import io\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "data_fname = 'training-data-set-1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadfile(fname):\n",
    "    \n",
    "    def iterfile(fname):\n",
    "        with io.open(fname,'r',encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                yield line\n",
    "\n",
    "    l = []\n",
    "    l_temp = []\n",
    "    \n",
    "    for line in iterfile(fname):\n",
    "        if line != '\\n':\n",
    "            try:\n",
    "                token,label = line.strip().split()\n",
    "                if label == '0':\n",
    "                    label = 'O'\n",
    "            except ValueError:\n",
    "                print(idx)\n",
    "            l_temp.append((token,label))\n",
    "        else:\n",
    "            l.append(l_temp)\n",
    "            l_temp = []\n",
    "            \n",
    "    return l\n",
    "\n",
    "\n",
    "def word2features(sent, i):\n",
    "    \n",
    "    word=sent[i][0]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper': word.isupper(),\n",
    "        'word.istitle': word.istitle(),\n",
    "        'word.isdigit': word.isdigit(),\n",
    "        'BOS': False,\n",
    "        'EOS': False,\n",
    "        '-1:word.lower': '<pad>',\n",
    "        '-1:word.istitle': False,\n",
    "        '-1:word.isupper': False,\n",
    "        '+1:word.lower': '<pad>',\n",
    "        '+1:word.istitle': False,\n",
    "        '+1:word.isupper': False\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower': word1.lower(),\n",
    "            '-1:word.istitle': word1.istitle(),\n",
    "            '-1:word.isupper': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS']= True\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0] \n",
    "        features.update({\n",
    "            '+1:word.lower': word1.lower(),\n",
    "            '+1:word.istitle': word1.istitle(),\n",
    "            '+1:word.isupper': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS']= True\n",
    "        \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token,label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return[token for token,label in sent]\n",
    "\n",
    "def tran_test_split(data,per=0.2):\n",
    "    split = int(len(data)*(1-per))\n",
    "    train_sents = data[:split]\n",
    "    test_sents = data[split:]\n",
    "    return train_sents,test_sents\n",
    "\n",
    "def data2feats(sents):\n",
    "    X = [sent2features(s) for s in sents]\n",
    "    y = [sent2labels(s) for s in sents]\n",
    "    return X,y\n",
    "\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'3', u'B_amount'),\n",
       "  (u'Liter', u'B_unit'),\n",
       "  (u'frische', u'B_productName'),\n",
       "  (u'Bio', u'I_productName'),\n",
       "  (u'Vollmilch', u'I_productName'),\n",
       "  (u'mit', 'O'),\n",
       "  (u'1.5%', u'B_productAttribute'),\n",
       "  (u'Fett', u'I_productAttribute'),\n",
       "  (u'von', 'O'),\n",
       "  (u'Berchtesgadener', u'B_brand'),\n",
       "  (u'Land', u'I_brand')],\n",
       " [(u'2', u'B_amount'),\n",
       "  (u'Liter', u'B_unit'),\n",
       "  (u'frische', u'B_productName'),\n",
       "  (u'Bio', u'I_productName'),\n",
       "  (u'Vollmilch', u'I_productName'),\n",
       "  (u'mit', 'O'),\n",
       "  (u'3.8%', u'B_productAttribute'),\n",
       "  (u'Fett', u'I_productAttribute')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadfile(data_fname)\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sents: \t107\n",
      "Train Tokens: \t535\n",
      "- - - - - - -\n",
      "Test Sents: \t27\n",
      "Test Tokens: \t133\n",
      "\n",
      "Wall time: 6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sents,test_sents = tran_test_split(data, per=0.2)\n",
    "X_train,y_train = data2feats(train_sents)\n",
    "X_test,y_test = data2feats(test_sents)\n",
    "\n",
    "print('Train Sents: \\t%i\\n' %len(X_train) +\n",
    "      'Train Tokens: \\t%i\\n' %sum([len(x) for x in X_train]) +\n",
    "      '- - - - - - -\\n'\n",
    "      'Test Sents: \\t%i\\n' %len(X_test) +\n",
    "      'Test Tokens: \\t%i\\n' %sum([len(x) for x in X_test])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+1:word.istitle': True,\n",
       " '+1:word.isupper': False,\n",
       " '+1:word.lower': u'liter',\n",
       " '-1:word.istitle': False,\n",
       " '-1:word.isupper': False,\n",
       " '-1:word.lower': '<pad>',\n",
       " 'BOS': True,\n",
       " 'EOS': False,\n",
       " 'bias': 1.0,\n",
       " 'word.isdigit': True,\n",
       " 'word.istitle': False,\n",
       " 'word.isupper': False,\n",
       " 'word.lower': u'3',\n",
       " 'word[-2:]': u'3',\n",
       " 'word[-3:]': u'3'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # epoches\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 166 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('UG_NER_1203_1st.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7632a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('UG_NER_1203_1st.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent:\t\teine TÃ¼te Chio Tortillas Nacho Cheese\n",
      "\n",
      "Predict:\tB_amount B_unit B_brand I_brand B_productName I_productName\n",
      "Labels:\t\tB_amount B_unit B_brand B_productName I_productName B_productAttribute\n"
     ]
    }
   ],
   "source": [
    "example = test_sents[0]\n",
    "\n",
    "print('Sent:\\t\\t' + ' '.join(sent2tokens(example)) + '\\n')\n",
    "print('Predict:\\t' + ' '.join(tagger.tag(example)))\n",
    "print('Labels:\\t\\t' + ' '.join(sent2labels(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          B_amount       0.97      1.00      0.98        28\n",
      "           B_brand       0.50      0.89      0.64         9\n",
      "B_productAttribute       0.33      0.25      0.29         8\n",
      "     B_productName       0.57      0.53      0.55        30\n",
      "            B_unit       0.72      0.95      0.82        22\n",
      "           I_brand       0.67      0.50      0.57         4\n",
      "I_productAttribute       0.00      0.00      0.00         1\n",
      "     I_productName       0.56      0.29      0.38        17\n",
      "\n",
      "       avg / total       0.70      0.69      0.68       133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bio_classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
